{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857e222-e084-4267-a276-537047c6c093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install huggingface_hub\n",
    "#pip install pypdf\n",
    "#pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc56fe1-ada6-4f37-b8d9-9f3aa080fb09",
   "metadata": {},
   "source": [
    "## PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d667c0f-4e1a-4136-aa6d-26e5492fde4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27e39d3b-a3ad-4579-b2cb-40229bd772a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceHubEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "234831c4-064e-41c3-9963-351c29a2bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"C:\\\\Users\\\\Mahir Dursunoglu\\\\Desktop\\\\Langchain\\\\ae.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06c12c5d-628f-4c8c-9e94-0fedd47e3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa69b152-8b74-4ac4-b09b-3be38d479d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be10a7cb-f89e-469d-a7a7-abb9fe191499",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pages[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "450b5680-aca4-4b38-99bc-9601044f9a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTRODUCTION  \n",
      "3 \n",
      " genuinely understand what you are saying, and do not merely seem \n",
      "to myself to do so? ’ cannot be, on the face of it, answered by either \n",
      "of the two great instruments of human knowledge: empirical \n",
      "investigation on the one hand, and, on the other, deductive \n",
      "reasoning as it is used in the formal disciplines – the kind of \n",
      "argument which occurs, for example, in mathematics or logic or \n",
      "grammar.  \n",
      "Indeed it might almost be said that the history of philosophy in \n",
      "its relation to the sciences consists, in part, in the disentangling of \n",
      "those questions which are either empirical (and inductive) or \n",
      "formal (and deductive) from the mass of problems which fi ll the \n",
      "minds of men, and the sorting out of these under the heads of the \n",
      "empirical or formal sciences concerned with them. It is in this way \n",
      "that, for instance, astronomy, mathematics, psychology, biology \n",
      "and the rest became divorced from the general corpu s of \n",
      "philosophy (of which they once formed a part), and embarked \n",
      "upon fruitful careers of their own as independent disciplines. They \n",
      "remained within the province of philosophy only so long as the \n",
      "kinds of way in which their problems were to be settled rema ined \n",
      "unclear, so that they were liable to be confused with other \n",
      "problems with which they had relatively little in common, and their \n",
      "differences from which had not been sufficiently discerned. The \n",
      "advance both of the sciences and of philosophy seems bound up \n",
      "with this progressive allocation of the empirical and formal \n",
      "elements, each to its own proper sphere; always, however, leaving \n",
      "behind a nucleus of unresolved (and largely unanalysed) questions, \n",
      "whose generality, obscurity and, above all, apparent (or re al) \n",
      "insolubility by empirical or formal methods gives them a status of \n",
      "their own which we tend to call philosophical.  \n",
      "Realisation of this truth (if it be one) was a long time in arriving. \n",
      "The natural tendency was to regard philosophical questions as \n",
      "being on a level with other questions, and answerable by similar \n",
      "means; especially by means which had been successful in ans wering \n",
      "these other questions, which in fact did turn out to be either \n"
     ]
    }
   ],
   "source": [
    "print(page.page_content[0:2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80d0d6fb-1b3b-4d70-8f8f-8f77488b02bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'C:\\\\Users\\\\Mahir Dursunoglu\\\\Desktop\\\\Langchain\\\\ae.pdf',\n",
       " 'page': 50}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f60c4ce-b0a5-40e5-b153-d2ae7845172e",
   "metadata": {},
   "source": [
    "## Document Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb5b6079-8dae-4656-a156-2e8d88296e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad2316b7-5470-4e01-b1a6-ee47963de477",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2300,\n",
    "    chunk_overlap=100, \n",
    "    length_function = len,\n",
    "    separators=[\"\\n\\n\", \"\\n\",\"(?<=\\.\" \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7284041-9581-4f3c-8e96-57421dab443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits =  text_splitter.split_documents(pages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b6e62f2-7f6c-4ce7-be75-2158b27c1e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408489d5-d174-42ee-b9c4-c6e7e1e2866b",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47379a78-f645-4fd7-97f6-dc56b5deb771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "#hugging_face_token = \"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\"\n",
    "\n",
    "#embedding = HuggingFaceInferenceAPIEmbeddings(\n",
    "#    api_key=hugging_face_token, model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93ad3252-3359-4b15-929a-f9e2ffd03ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "#def get_embedding(text, model=\"PsiPi/liuhaotian_llava-v1.5-13b-GGUF\"):\n",
    "#   text = text.replace(\"\\n\", \" \")\n",
    "#   return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da1281b9-3b63-4018-896d-bd4448f0cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9868ba05-7514-4d8b-88a3-8bbbff875a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"king\"\n",
    "sentence2 = \"queen\"\n",
    "sentence3 = \"prince\"\n",
    "sentence4 = \"man\"\n",
    "sentence5 = \"woman\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e30047fa-eef7-4069-a563-6e89df6b62c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 2: 0.6807128\n",
      "score 3: 0.58839214\n",
      "score 4: 0.3216458\n",
      "score 5: 0.2639951\n"
     ]
    }
   ],
   "source": [
    "embedding1 = model.encode(sentence1)\n",
    "embedding2 = model.encode(sentence2)\n",
    "embedding3 = model.encode(sentence3)\n",
    "embedding4 = model.encode(sentence4)\n",
    "embedding5 = model.encode(sentence5)\n",
    "\n",
    "print(\"score 2:\",np.dot(embedding1,embedding2))\n",
    "print(\"score 3:\",np.dot(embedding1,embedding3))\n",
    "print(\"score 4:\",np.dot(embedding1,embedding4))\n",
    "print(\"score 5:\",np.dot(embedding1,embedding5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01071e3-082c-43d9-b35c-8e23f08b8681",
   "metadata": {},
   "source": [
    "## Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcb9a7f8-d1b7-441f-b728-4daf50d8deb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dcdc4de1-c554-4d83-9dbe-ec302404edc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Delete of nonexisting embedding ID: id1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9917622c-75e0-430d-b125-6262c1ef5e78",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Document' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:592\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_module()\u001b[38;5;241m.\u001b[39mtokenize(texts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;66;03m# In case some Module does not allow for kwargs in tokenize, we also try without any\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:134\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[1;34m(self, texts, padding)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_tuple \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m--> 134\u001b[0m     batch1\u001b[38;5;241m.\u001b[39mappend(text_tuple[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    135\u001b[0m     batch2\u001b[38;5;241m.\u001b[39mappend(text_tuple[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Document' object is not subscriptable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 20\u001b[0m\n\u001b[0;32m     12\u001b[0m mbedding_func \u001b[38;5;241m=\u001b[39m embedding_functions\u001b[38;5;241m.\u001b[39mSentenceTransformerEmbeddingFunction(model_name\u001b[38;5;241m=\u001b[39mEMBED_MODEL)\n\u001b[0;32m     14\u001b[0m collection \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mcreate_collection(\n\u001b[0;32m     15\u001b[0m     name\u001b[38;5;241m=\u001b[39mCOLLECTION_NAME,\n\u001b[0;32m     16\u001b[0m     embedding_function\u001b[38;5;241m=\u001b[39membedding_func,\n\u001b[0;32m     17\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhnsw:space\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     18\u001b[0m )\n\u001b[1;32m---> 20\u001b[0m collection\u001b[38;5;241m.\u001b[39madd(documents\u001b[38;5;241m=\u001b[39msplits,\n\u001b[0;32m     21\u001b[0m     ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(splits))]\n\u001b[0;32m     22\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:154\u001b[0m, in \u001b[0;36mCollection.add\u001b[1;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# At this point, we know that one of documents or images are provided from the validation above\u001b[39;00m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mdocuments)\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    156\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mimages)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:633\u001b[0m, in \u001b[0;36mCollection._embed\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must provide an embedding function to compute embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.trychroma.com/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    632\u001b[0m     )\n\u001b[1;32m--> 633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\chromadb\\api\\types.py:193\u001b[0m, in \u001b[0;36mEmbeddingFunction.__init_subclass__.<locals>.__call__\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m: EmbeddingFunction[D], \u001b[38;5;28minput\u001b[39m: D) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embeddings:\n\u001b[1;32m--> 193\u001b[0m     result \u001b[38;5;241m=\u001b[39m call(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate_embeddings(maybe_cast_one_to_many_embedding(result))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\chromadb\\utils\\embedding_functions.py:79\u001b[0m, in \u001b[0;36mSentenceTransformerEmbeddingFunction.__call__\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Documents) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embeddings:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m     78\u001b[0m         Embeddings,\n\u001b[1;32m---> 79\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mencode(\n\u001b[0;32m     80\u001b[0m             \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m),\n\u001b[0;32m     81\u001b[0m             convert_to_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     82\u001b[0m             normalize_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_embeddings,\n\u001b[0;32m     83\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m     84\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:366\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[0;32m    365\u001b[0m     sentences_batch \u001b[38;5;241m=\u001b[39m sentences_sorted[start_index : start_index \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m--> 366\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(sentences_batch)\n\u001b[0;32m    367\u001b[0m     features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    368\u001b[0m     features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:595\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_module()\u001b[38;5;241m.\u001b[39mtokenize(texts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;66;03m# In case some Module does not allow for kwargs in tokenize, we also try without any\u001b[39;00m\n\u001b[1;32m--> 595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_module()\u001b[38;5;241m.\u001b[39mtokenize(texts)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:134\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[1;34m(self, texts, padding)\u001b[0m\n\u001b[0;32m    132\u001b[0m batch1, batch2 \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_tuple \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m--> 134\u001b[0m     batch1\u001b[38;5;241m.\u001b[39mappend(text_tuple[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    135\u001b[0m     batch2\u001b[38;5;241m.\u001b[39mappend(text_tuple[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    136\u001b[0m to_tokenize \u001b[38;5;241m=\u001b[39m [batch1, batch2]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Document' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=EMBED_MODEL)\n",
    "\n",
    "CHROMA_DATA_PATH = \"C:\\\\Users\\\\Mahir Dursunoglu\\\\Desktop\\\\Langchain\\\\Vectorstore\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "COLLECTION_NAME = \"demo_docs2\"\n",
    "\n",
    "client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)\n",
    "\n",
    "mbedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=EMBED_MODEL)\n",
    "\n",
    "collection = client.create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    embedding_function=embedding_func,\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "\n",
    "collection.add(documents=splits,\n",
    "    ids=[f\"id{i}\" for i in range(len(splits))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5f7ddac-a7b5-42cb-b709-3257d16592b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [],\n",
       " 'documents': [],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5be3de85-1754-4c79-b2d8-bb9e0070ef39",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SentenceTransformerEmbeddingFunction' object has no attribute 'embed_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m persist_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mMahir Dursunoglu\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mLangchain\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mVectorstore\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m embedding_func \u001b[38;5;241m=\u001b[39m embedding_functions\u001b[38;5;241m.\u001b[39mSentenceTransformerEmbeddingFunction(model_name\u001b[38;5;241m=\u001b[39mEMBED_MODEL)\n\u001b[1;32m----> 7\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[0;32m      8\u001b[0m     documents\u001b[38;5;241m=\u001b[39msplits,\n\u001b[0;32m      9\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membedding_func,\n\u001b[0;32m     10\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:778\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    776\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    777\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[0;32m    779\u001b[0m     texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    780\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membedding,\n\u001b[0;32m    781\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m    782\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    783\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[0;32m    784\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory,\n\u001b[0;32m    785\u001b[0m     client_settings\u001b[38;5;241m=\u001b[39mclient_settings,\n\u001b[0;32m    786\u001b[0m     client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[0;32m    787\u001b[0m     collection_metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    789\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:736\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[0;32m    731\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,\n\u001b[0;32m    732\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    733\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m    734\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    735\u001b[0m     ):\n\u001b[1;32m--> 736\u001b[0m         chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(\n\u001b[0;32m    737\u001b[0m             texts\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m [],\n\u001b[0;32m    738\u001b[0m             metadatas\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    739\u001b[0m             ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    740\u001b[0m         )\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    742\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:275\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 275\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    279\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SentenceTransformerEmbeddingFunction' object has no attribute 'embed_documents'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "persist_directory = \"C:\\\\Users\\\\Mahir Dursunoglu\\\\Desktop\\\\Langchain\\\\Vectorstore\"\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=EMBED_MODEL)\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf7c3860-d24f-4960-952b-025725a65a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1611\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d310b12-4f9f-484b-bcd6-79c059958dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 179, 'source': 'C:\\\\Users\\\\Mahir Dursunoglu\\\\Desktop\\\\Langchain\\\\ae.pdf'}\n",
      "{'page': 192, 'source': 'C:\\\\Users\\\\Mahir Dursunoglu\\\\Desktop\\\\Langchain\\\\ae.pdf'}\n"
     ]
    }
   ],
   "source": [
    "question = \"what is the most prominent idea of Berkeley on human mind\"\n",
    "docs = vectordb.max_marginal_relevance_search(question,k=2, fetch_k=10)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "292a1df9-cf20-4c01-bec7-1e3db5c4c925",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEORGE BERKELEY  \n",
      "132 \n",
      " mathematical concepts ’, or ‘values ’, or the like. And this leads to \n",
      "the assumption of many strange worlds. Berkeley is one of the first \n",
      "philosophers to point out that language is used for many purposes \n",
      "besides that of describing; and that to look for objective \n",
      "counterparts , in the same sense, to all varieties of sen tences  leads \n",
      "to many fertile inventions of fictional entities, and conceals plain \n",
      "facts under a rich mythology. This too is an intellectual service of \n",
      "the first order.  [132]  \n",
      " \n",
      "OF THE PR INCIPLES OF HUMAN KNOWLEDGE, PART I  \n",
      "1. It is evident to any one who takes a survey of the objects of human \n",
      "knowledge, that they are either ideas actually imprinted on the senses, or \n",
      "else such as are perceived by attending to the passions and operations of \n",
      "the mind, or lastly ideas formed by he lp of memory and imagination, \n",
      "either compounding, dividing, or barely representing those originally \n",
      "perceived in the aforesaid ways. By sight I have the ideas of light and \n",
      "colours with their several degrees and variations. By touch I perceive, for \n",
      "example,  hard and soft, heat and cold, motion and resistance, and of all \n",
      "these more and less either as to quantity or degree. Smelling furnishes \n",
      "me with odours; the palate with tastes, and hearing conveys sounds to the \n",
      "mind in all their variety of tone and composi tion. And as several of these \n",
      "are observed to accompany each other, they come to be marked by one \n",
      "name, and so to be reputed as one thing. Thus, for example, a certain \n",
      "colour, taste, smell, figure and consistence having been observed to go \n",
      "together, are ac counted one distinct thing, signified by the name  apple.  \n",
      "Other collections of ideas constitute a stone, a tree, a book, and the like \n",
      "sensible things; which, as they are pleasing or disagreeable, excite the \n",
      "passions of love, hatred, joy, grief, and so forth.  \n",
      "2. But besides all that endless variety of ideas or objects of knowledge, \n",
      "there is likewise something which knows or perceives them, and exercises \n",
      "divers operations, as willing, imagining, remembering about them. This \n",
      "perceiving, active being is what  I call  mind, spirit, soul  or myself . By which \n",
      "words I do not denote any one of my ideas, but a thing entirely distinct\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee73956a-23d9-4bee-9188-8c0696de7d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1db3ba7-e131-4b09-a40d-810925b91d00",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb31f9-597c-4698-afd2-d9f07baaa036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: reuse your existing OpenAI setup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "context = docs[0].page_content\n",
    "\n",
    "\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbafc5f2-5685-446b-ab4e-6c62c1bf0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"PsiPi/liuhaotian_llava-v1.5-13b-GGUF\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Answer in simple terms and be nice\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the difference between cat and dog?\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdf92b32-36e4-4c81-997b-1eac5cd07246",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectordb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vectordb\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mcount()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vectordb' is not defined"
     ]
    }
   ],
   "source": [
    "vectordb._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570bb3d4-fd9c-4078-ae60-99c50083d938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c05915ca-9243-45e4-b1bb-ba3627ec5e2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HuggingFaceEndpoint\n__root__\n  Could not authenticate with huggingface_hub. Please check your API token. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEndpoint\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatHuggingFace\n\u001b[1;32m----> 4\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFaceEndpoint(repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFaceH4/zephyr-7b-beta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m chat_model \u001b[38;5;241m=\u001b[39m ChatHuggingFace(llm\u001b[38;5;241m=\u001b[39mllm)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\load\\serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for HuggingFaceEndpoint\n__root__\n  Could not authenticate with huggingface_hub. Please check your API token. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import \n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id=\"HuggingFaceH4/zephyr-7b-beta\")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617b31a1-fde5-4aab-b711-b48649dbb13a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
